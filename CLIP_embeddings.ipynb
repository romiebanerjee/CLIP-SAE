{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/romiebanerjee/CLIP-SAE/blob/master/CLIP_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5Vx8NE3nFMd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80ac9870-fab7-43a0-b247-9ff4a61a3997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "# !pip install -q transformers torch torchvision pillow\n",
        "# !pip install -q git+https://github.com/openai/CLIP.git\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
        "\n",
        "# Check if GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model_names = [\"openai/clip-vit-base-patch32\", \"openai/clip-vit-large-patch14\", \"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IM3uNqQI2_lZ"
      },
      "outputs": [],
      "source": [
        "model_name = model_names[0]\n",
        "model = CLIPModel.from_pretrained(model_name)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)\n",
        "tokenizer = CLIPTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0nXcyOocOYm",
        "outputId": "7087e72f-a698-49db-c511-1ef06a2468db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9949, 0.0051]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "outputs = model(**inputs)\n",
        "logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
        "probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n",
        "print(probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTMkVr6NY9LM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "522a6b12-84b1-4177-8c89-deeccf45c0ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `hf-access` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `hf-access`\n"
          ]
        }
      ],
      "source": [
        "!hf auth login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8k5RfoRXgoek"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# # Stream the dataset without downloading it entirely\n",
        "# ds = load_dataset(\"jp1924/Laion400m-3\", streaming=True, split=\"train\")\n",
        "\n",
        "# # Take a small subset (e.g., 1000 samples)\n",
        "# # small_ds = ds.take(10000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# # Stream from a single Parquet file (local or remote)\n",
        "# ds_00000_of_00449 = load_dataset(\"parquet\", data_files=\"https://huggingface.co/datasets/jp1924/Laion400m-3/resolve/main/data/train-00000-of-00449.parquet\", streaming=True, split=\"train\")\n",
        "# ds_00001_of_00449 = load_dataset(\"parquet\", data_files=\"https://huggingface.co/datasets/jp1924/Laion400m-3/resolve/main/data/train-00001-of-00449.parquet\", streaming=True, split=\"train\")\n",
        "# ds_00002_of_00449 = load_dataset(\"parquet\", data_files=\"https://huggingface.co/datasets/jp1924/Laion400m-3/resolve/main/data/train-00002-of-00449.parquet\", streaming=True, split=\"train\")\n",
        "# ds_00003_of_00449 = load_dataset(\"parquet\", data_files=\"https://huggingface.co/datasets/jp1924/Laion400m-3/resolve/main/data/train-00003-of-00449.parquet\", streaming=True, split=\"train\")\n",
        "ds_00004_of_00449 = load_dataset(\"parquet\", data_files=\"https://huggingface.co/datasets/jp1924/Laion400m-3/resolve/main/data/train-00004-of-00449.parquet\", streaming=True, split=\"train\")\n",
        "ds_00005_of_00449 = load_dataset(\"parquet\", data_files=\"https://huggingface.co/datasets/jp1924/Laion400m-3/resolve/main/data/train-00005-of-00449.parquet\", streaming=True, split=\"train\")"
      ],
      "metadata": {
        "id": "yAvTrFipFRoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, sample in enumerate(ds_00005_of_00449):\n",
        "    print(idx, sample)\n",
        "    print(sample['caption'])\n",
        "    display(sample['image'])\n",
        "    if idx > 10:\n",
        "        break"
      ],
      "metadata": {
        "collapsed": true,
        "id": "X71o3fbZDQvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text = tokenizer.decode(tokenizer(\"hello Ich bin \")['input_ids'][:3])\n",
        "# text.replace(\"<|startoftext|>\", \"\").replace(\"<|endoftext|>\", \"\").strip()\n",
        "max_position_embs = model.text_model.config.max_position_embeddings"
      ],
      "metadata": {
        "id": "imZkv2CnikCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_text(text):\n",
        "\n",
        "    tokens = tokenizer(text)['input_ids'][:max_position_embs-2]\n",
        "    clipped_text = tokenizer.decode(tokens)\n",
        "    # print(clipped_text)\n",
        "    clipped_text = clipped_text.replace(\"<|startoftext|>\", \"\").replace(\"<|endoftext|>\", \"\").strip()\n",
        "    return clipped_text\n",
        "\n",
        "clip_text(\"a phot o dfds;sd flsdflsd flksjdfls dfjs jfdlksjd flsjfljsdf dslfdslj fldskjf ls\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5GNBSC1yiGlU",
        "outputId": "a4c7d63a-e467-492a-f806-a05106f59faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a phot o dfds ; sd flsdflsd flksjdfls dfjs jfdlksjd flsjfljsdf dslfdslj fldskjf ls'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJmHZXMMGiSi"
      },
      "outputs": [],
      "source": [
        "def get_image_embedding(input):\n",
        "    if isinstance(input, str):\n",
        "    # Check if it is a url\n",
        "      image = Image.open(requests.get(input, stream=True).raw)\n",
        "    # Check if it's a JPEG file (file-like object or bytes)\n",
        "    elif hasattr(input, 'read') or isinstance(input, bytes):\n",
        "        # You might want to add more checks here to verify it's a JPEG\n",
        "        image = Image.open(input)\n",
        "    # Check if it's a PIL Image object\n",
        "    elif isinstance(input, Image.Image):\n",
        "        image = input\n",
        "    else:\n",
        "        return \"Unknown type\"\n",
        "\n",
        "    try:\n",
        "      # image = Image.open(image)\n",
        "      # display(image)\n",
        "      inputs = processor(images=image, return_tensors=\"pt\", padding=True).to(device)\n",
        "      with torch.no_grad():\n",
        "        outputs = model.vision_model(**inputs)\n",
        "        outputs = outputs.pooler_output\n",
        "        image_embeds = model.visual_projection(outputs)\n",
        "        image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        return image_embeds.cpu()\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image: {image}\")\n",
        "        print(f\"Error message: {str(e)}\")\n",
        "        return torch.zeros((1, 512)).to(device)\n",
        "\n",
        "def get_text_embedding(text):\n",
        "    text = clip_text(text)\n",
        "    inputs = processor(text=text, return_tensors=\"pt\", padding=True).to(device)\n",
        "    try:\n",
        "      with torch.no_grad():\n",
        "        outputs = model.text_model(**inputs)\n",
        "        outputs = outputs.pooler_output\n",
        "        text_embeds = model.text_projection(outputs)\n",
        "        text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing text: {text}\")\n",
        "        print(f\"Error message: {str(e)}\")\n",
        "        return torch.zeros((1, 512)).to(device)\n",
        "\n",
        "    return text_embeds.cpu()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "coaJ6aFwNh26",
        "outputId": "1f526663-5e49-4a4b-e8b8-4e1ef64658de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "155898it [4:15:59, 10.15it/s]\n"
          ]
        }
      ],
      "source": [
        "dataset_name = ds_00005_of_00449\n",
        "from tqdm  import tqdm\n",
        "image_embeds = []\n",
        "text_embeds = []\n",
        "for idx, sample in tqdm(enumerate(dataset_name)):\n",
        "    # if idx > 1000:\n",
        "    #     break\n",
        "    image_emb = get_image_embedding(sample['image'])\n",
        "    caption_emb = get_text_embedding(sample['caption'])\n",
        "    image_embeds.append(image_emb)\n",
        "    text_embeds.append(caption_emb)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_embeddings = torch.stack(image_embeds)\n",
        "text_embeddings = torch.stack(text_embeds)"
      ],
      "metadata": {
        "id": "bLDpAG5aaV8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "torch.save(image_embeddings, '/content/drive/MyDrive/data/CLIP_embeddings/image_embeddings_00005-00449.pt')\n",
        "torch.save(text_embeddings, '/content/drive/MyDrive/data/CLIP_embeddings/text_embeddings_00005-00449.pt')"
      ],
      "metadata": {
        "id": "27ujvaUWZbXI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMlKp7IgEpJHhdSUfw0EQ6+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}